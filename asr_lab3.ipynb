{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5970bd3f-7b36-46bc-bcb6-82c1f9ff36bc",
   "metadata": {},
   "source": [
    "# Акустические модели\n",
    "\n",
    "Акустическая модель - это часть системы автоматического распознавания речи, которая используется для преобразования аудиосигнала речи в последовательность фонем или других единиц речевого звука. Акустическая модель обучается на большом наборе речевых данных, чтобы определить, какие звуки соответствуют конкретным акустическим признакам в аудиосигнале. Эта модель может использоваться вместе с другими компонентами, такими как языковая модель и модель декодирования, чтобы достичь более точного распознавания речи.\n",
    "\n",
    "В данной работе мы сконцентрируемся на обучении нейросетевых акустических моделей с помощью библиотек torch и torchaudio. Для экспериментов будем использовать базу [TIMIT](https://catalog.ldc.upenn.edu/LDC93s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "from typing import List, Dict, Union, Set, Any\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d611e67-aec1-4714-a722-865ec980d83d",
   "metadata": {},
   "source": [
    "# Загрузка датасета TIMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e498e1d-a102-42a3-bf67-7389d89bb483",
   "metadata": {},
   "source": [
    "Официальная страница датасета TIMIT \n",
    "\n",
    "Для простоты загрузки данных удобнее всего пользоваться копией датасета, выложенной на kaggle \n",
    "\n",
    "https://www.kaggle.com/datasets/mfekadu/darpa-timit-acousticphonetic-continuous-speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb80729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Kaggle/kaggle-api - Docs kaggle \n",
    "# Simplest way: go to https://www.kaggle.com/settings , \"Create new token\" and move it into \"~/.kaggle\"\n",
    "\n",
    "#!kaggle datasets download -d mfekadu/darpa-timit-acousticphonetic-continuous-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ded70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip -o -q darpa-timit-acousticphonetic-continuous-speech.zip -d timit/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bed5129-a418-463b-881b-8e0d10ac4ba7",
   "metadata": {},
   "source": [
    "# 1. Подготовка данных для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f0891-77c1-4e55-a60d-89d5890796bc",
   "metadata": {},
   "source": [
    "TIMIT является одной из самых широко используемых баз данных для изучения систем автоматического распознавания речи. База данных TIMIT содержит произнесения предложений различными дикторами. Каждое произнесение сопровождается его словной и фонетической разметкой.\n",
    "\n",
    "Для обучения акустической модели нам в первую очередь интересна фонетическая разметка произнесений. Такая разметка сопоставляет фонемы, которые были произнесены диктором, с временными интервалами в записи. Такая разметка позволит нам обучить пофреймовый классификатор, который будет предсказывать сказанную фонему."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df01d01-7bde-4236-8ce0-4983ba3b18e0",
   "metadata": {},
   "source": [
    "## 1.a. Загрузка базы с диска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimitDataset(Dataset):\n",
    "    \"\"\"Загрузка TIMIT данных с диска\"\"\"\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.uri2wav = {}\n",
    "        self.uri2text = {}\n",
    "        self.uri2word_ali = {}\n",
    "        self.uri2phone_ali = {}\n",
    "        for d, _, fs in os.walk(data_path):\n",
    "            for f in fs:\n",
    "                full_path = f'{d}/{f}'\n",
    "                if f.endswith('.WAV'):\n",
    "                    # skip it. Use .wav instead\n",
    "                    pass\n",
    "                elif f.endswith('.wav'):\n",
    "                    stem = Path(f[:-4]).stem # .WAV.wav\n",
    "                    self.uri2wav[f'{d}/{stem}'] = full_path\n",
    "                elif f.endswith('.TXT'):\n",
    "                    stem = Path(f).stem\n",
    "                    self.uri2text[f'{d}/{stem}'] = full_path\n",
    "                elif f.endswith('.WRD'):\n",
    "                    stem = Path(f).stem\n",
    "                    self.uri2word_ali[f'{d}/{stem}'] = full_path\n",
    "                elif f.endswith('.PHN'):\n",
    "                    stem = Path(f).stem\n",
    "                    self.uri2phone_ali[f'{d}/{stem}'] = full_path\n",
    "                else:\n",
    "                    warnings.warn(f\"Unknown file type {full_path} . Skip it.\")\n",
    "        \n",
    "        self.uris = list(sorted(set(self.uri2wav.keys()) \\\n",
    "                                & set(self.uri2text.keys()) \\\n",
    "                                & set(self.uri2word_ali.keys()) \\\n",
    "                                &  set(self.uri2phone_ali.keys())\n",
    "                               ))\n",
    "        print(f\"Found {len(self.uris)} utterances in {self.data_path}. \", \n",
    "              f\"{len(self.uri2wav)} wavs, \", \n",
    "              f\"{len(self.uri2text)} texts, \",\n",
    "              f\"{len(self.uri2word_ali)} word alinments, \",\n",
    "             f\"{len(self.uri2phone_ali)} phone alignments\")\n",
    "    \n",
    "    def get_uri(self, index_or_uri: Union[str, int]):\n",
    "        if isinstance(index_or_uri, str):\n",
    "            uri = index_or_uri\n",
    "        else:\n",
    "            uri = self.uris[index_or_uri]\n",
    "        return uri\n",
    "    \n",
    "    \n",
    "    def get_audio(self, index_or_uri: Union[str, int]):\n",
    "        uri = self.get_uri(index_or_uri)\n",
    "        wav_path = self.uri2wav[uri]\n",
    "        wav_channels, sr = torchaudio.load(wav_path)\n",
    "        return wav_channels[0], sr \n",
    "        \n",
    "    def get_text(self, index_or_uri: Union[str, int]):\n",
    "        \"\"\" Return (start_sample, stop_sample, text)\"\"\"\n",
    "        uri = self.get_uri(index_or_uri)\n",
    "        txt_path = self.uri2text[uri]\n",
    "        with open(txt_path) as f:\n",
    "            start, stop, text = f.read().strip().split(maxsplit=2)\n",
    "            start, stop = int(start), int(stop)\n",
    "            assert start == 0, f\"{txt_path}\"\n",
    "        return start, stop, text\n",
    "    \n",
    "    def get_word_ali(self, index_or_uri):\n",
    "        \"\"\" Return [(start_sample, stop_sample, word), ...]\"\"\"\n",
    "        uri = self.get_uri(index_or_uri)\n",
    "        wrd_path = self.uri2word_ali[uri]\n",
    "        with open(wrd_path) as f:\n",
    "            words = [(int(start), int(stop), word) for start, stop, word in map(str.split, f.readlines())]\n",
    "        return words\n",
    "    \n",
    "    def get_phone_ali(self, index_or_uri):\n",
    "        \"\"\" Return [(start_sample, stop_sample, phone), ...]\"\"\"\n",
    "        uri = self.get_uri(index_or_uri)\n",
    "        ph_path = self.uri2phone_ali[uri]\n",
    "        with open(ph_path) as f:\n",
    "            phonemes = [(int(start), int(stop), ph) for start, stop, ph in map(str.split, f.readlines())]\n",
    "        return phonemes\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {\"uri\": self.get_uri(index),\n",
    "                \"audio\": self.get_audio(index),\n",
    "                \"text\": self.get_text(index),\n",
    "                \"word_ali\": self.get_word_ali(index),\n",
    "                \"phone_ali\": self.get_phone_ali(index)}       \n",
    "\n",
    "    def __len__(self):\n",
    "        # вернем количество доступных ури в выборке\n",
    "        return len(self.uris)\n",
    "\n",
    "    def total_audio_samples(self) -> int:\n",
    "        # сумма длины всех аудио в сэмплах\n",
    "        total = 0\n",
    "        for uri in self.uris:\n",
    "            wav, sr = self.get_audio(uri)\n",
    "            total += wav.shape[0]\n",
    "        return total\n",
    "\n",
    "    def total_num_words(self) -> int:\n",
    "        # сумма количества слов в словном выравнивании по всем ури\n",
    "        total = 0\n",
    "        for uri in self.uris:\n",
    "            total += len(self.get_word_ali(uri))\n",
    "        return total\n",
    "\n",
    "    def total_num_phones(self) -> int:\n",
    "        # сумма количества фонем во всех выравниваниях\n",
    "        total = 0\n",
    "        for uri in self.uris:\n",
    "            total += len(self.get_phone_ali(uri))\n",
    "        return total\n",
    "\n",
    "    def get_vocab(self) -> Set[str]:\n",
    "        # собрать множество уникальных слов из всех выравниваний\n",
    "        vocab = set()\n",
    "        for uri in self.uris:\n",
    "            words = self.get_word_ali(uri)\n",
    "            for _, _, w in words:\n",
    "                vocab.add(w)\n",
    "        return vocab\n",
    "\n",
    "    def get_phones(self) -> Set[str]:\n",
    "        # собрать множество уникальных фонем\n",
    "        phones = set()\n",
    "        for uri in self.uris:\n",
    "            phs = self.get_phone_ali(uri)\n",
    "            for _, _, p in phs:\n",
    "                phones.add(p)\n",
    "        return phones\n",
    "\n",
    "    def phones_prior(self) -> Dict[str, float]:\n",
    "        # априорные вероятности фонем по частотам появления\n",
    "        counts = Counter()\n",
    "        total = 0\n",
    "        for uri in self.uris:\n",
    "            phs = self.get_phone_ali(uri)\n",
    "            counts.update(p for _, _, p in phs)\n",
    "            total += len(phs)\n",
    "        priors = {ph: cnt / total for ph, cnt in counts.items()}\n",
    "        return priors\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4195f3-a0a6-4e13-83a2-6f96f0e99ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_timit_dataset_stats():\n",
    "    test_ds = TimitDataset('timit/data/TEST/')\n",
    "\n",
    "    print(\"Len\")\n",
    "    assert len(test_ds) == 1680, f\"{len(test_ds)}\"\n",
    "\n",
    "    print(\"Audio\")\n",
    "    audio_len = test_ds.total_audio_samples()\n",
    "    assert audio_len == 82986452, f\"{audio_len}\"\n",
    "\n",
    "    print(\"Words\")\n",
    "    words_len = test_ds.total_num_words()\n",
    "    assert words_len == 14553, f\"{words_len}\"\n",
    "\n",
    "    print(\"Phones\")\n",
    "    phones_len = test_ds.total_num_phones()\n",
    "    assert phones_len == 64145, f\"{phones_len}\"\n",
    "\n",
    "    print(\"Vocab\")\n",
    "    vocab = test_ds.get_vocab()\n",
    "    assert len(set(vocab)) == 2378, f\"{len(set(vocab))}\"\n",
    "\n",
    "    print(\"Phones vocab\")\n",
    "    phones = test_ds.get_phones()\n",
    "    assert len(set(phones)) == 61, f\"{len(set(phones))}\"\n",
    "    \n",
    "    print(\"Phones prior\")\n",
    "    priors = test_ds.phones_prior()\n",
    "    assert np.isclose(sum(priors.values()), 1.0), f\"sum(priors.values())\"\n",
    "    pmin, pmax = min(priors.keys(), key=priors.get), max(priors.keys(), key=priors.get)\n",
    "    assert pmin == 'eng', pmin\n",
    "    assert pmax == 'h#', pmax\n",
    "    print(\"Test 1.a passed\")\n",
    "test_timit_dataset_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eaa226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ds = TimitDataset('timit/data/TEST/')\n",
    "item = test_ds[5]\n",
    "print(item['uri'])\n",
    "print(item['text'][2])\n",
    "display.display(display.Audio(item['audio'][0].numpy(), rate=item['audio'][1]))\n",
    "print('---words---')\n",
    "for start, stop, word in item['word_ali']:\n",
    "    print(word)\n",
    "    display.display(display.Audio(item['audio'][0][start:stop].numpy(), rate=item['audio'][1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e16fa85-78d8-4604-a55d-377b197ec001",
   "metadata": {},
   "source": [
    "## 1.b. Экстрактор фич\n",
    "Для того чтобы построить акустическую модель, первым делом надо извлечь признаки аудио сигнала. Для распознавания речь принято использовать fbank признаки. fbank/MelSpectrogram признаки получается из амплитудного спектра сигнала путем свертки спекта с треугольными фильтрами в мел-шкале. Есть множество реализаций данных признаков в различных библиотеках (kaldi, librosa, torchaudio) и все они имеют свои особенности. В данной работе мы будем использовать реализацию из библиотеки torchaudio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e8d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_rate=16000,\n",
    "        n_fft=400,\n",
    "        hop_length=160,\n",
    "        n_mels=40,\n",
    "        f_max=7600,\n",
    "        spec_aug_max_fmask=80,\n",
    "        spec_aug_max_tmask=80,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        #TODO\n",
    "        # инициализируйте обработчик fbank фич из torchaudio\n",
    "        # self.mel_spec = ...\n",
    "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels,\n",
    "            f_max=f_max\n",
    "        )\n",
    "        \n",
    "    def samples2frames(self, num_samples: int) -> int:\n",
    "        # TODO\n",
    "        # Верните количество кадров в спектрограмме, соответствующей вавке длиной num_samples\n",
    "        # mel_spec с center=True добавляет паддинг n_fft//2 с обеих сторон\n",
    "        # формула для длины выходного спектра:\n",
    "        # frames = floor((num_samples + 2 * pad - n_fft) / hop_length) + 1\n",
    "        # где pad = n_fft // 2\n",
    "        pad = self.n_fft // 2\n",
    "        if num_samples + 2 * pad < self.n_fft:\n",
    "            return 0\n",
    "        return 1 + (num_samples + 2 * pad - self.n_fft) // self.hop_length\n",
    "    \n",
    "    @property\n",
    "    def feats_dim(self):\n",
    "        # TODO\n",
    "        # Верните количество извлекаемых фич\n",
    "        # размерность признаки = количество мел-банков\n",
    "        return self.n_mels \n",
    "    \n",
    "    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        mel = self.mel_spec(waveform)\n",
    "        return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d861c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_samples2frames():\n",
    "    fe = FeatureExtractor()\n",
    "    for i in tqdm(range(15000, 40000)):\n",
    "        wav = torch.zeros(i)\n",
    "        feats = fe(wav)\n",
    "        assert feats.shape[-2] == fe.feats_dim, f\"{i} {feats.shape[-2]=}, {fe.feats_dim}\"\n",
    "\n",
    "        assert feats.shape[-1] == fe.samples2frames(i), f\"{i} {feats.shape[-1]=}, {fe.samples2frames(i)}\"\n",
    "        \n",
    "    print('Test 1.b passed')\n",
    "test_samples2frames()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c53730-3c9e-495e-bf5d-dd258d879841",
   "metadata": {},
   "source": [
    "## 1.с. Таргеты и объединение данных в батчи \n",
    "\n",
    "Акустическая Модель (АМ) - пофреймовый классификатор, который предсказывает фонему для каждого кадра аудио. Для обучения AM будем использовать фонемное выравнивание. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd07a3ef-8dd3-4411-96fe-42d597a8d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TimitDataset('timit/data/TRAIN/')\n",
    "print(train_ds[0])\n",
    "\n",
    "# Строим мапинг из написания фонемы в ее id \n",
    "phones = train_ds.get_phones() \n",
    "phones.remove('pau')\n",
    "phones.remove('epi')\n",
    "phones.remove('h#')\n",
    "\n",
    "# Фонемы паузы должны иметь индекс 0\n",
    "PHONE2ID = {p:i for i, p in enumerate(['pau'] + list(sorted(phones)))}\n",
    "PHONE2ID['epi'] = 0\n",
    "PHONE2ID['h#'] = 0\n",
    "print(PHONE2ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6408ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatsPhoneDataset(TimitDataset):\n",
    "    def __init__(self, data_path, feature_extractor: FeatureExtractor, phone2id):\n",
    "        super().__init__(data_path)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.phone2id = phone2id\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        orig_item = super().__getitem__(index)\n",
    "        wav, sr = orig_item['audio']\n",
    "        assert sr == self.feature_extractor.sample_rate, f\"wrong sr for {index}\"\n",
    "        # подготавливаем пофреймовые фичи\n",
    "        feats = self.feature_extractor(wav)\n",
    "        feats = feats.squeeze(dim=0).transpose(0, 1) # time x feats\n",
    "\n",
    "        # создаем пофреймовое выравнивание \n",
    "        targets = torch.zeros(feats.shape[0], dtype=torch.long)\n",
    "        # TODO \n",
    "        # заполните пофреймовое фонемное выравнивание targets idшниками фонем\n",
    "        # используйте phone_ali \n",
    "        phone_ali = orig_item['phone_ali']\n",
    "        \n",
    "        # Чтобы заполнить пофреймовое выравнивание, для каждого кадра определяем фонему\n",
    "        # Фреймы получены с шагом hop_length, преобразуем sample индексы в индексы фреймов\n",
    "        frames_per_second = self.feature_extractor.sample_rate / self.feature_extractor.hop_length\n",
    "\n",
    "        # Пройдёмся по алигнменту фонемами\n",
    "        # Для каждого (start_sample, stop_sample, ph), найдём соответствующие индексы фреймов и проставим targets\n",
    "        for start_sample, stop_sample, ph in phone_ali:\n",
    "            start_frame = int(start_sample / self.feature_extractor.hop_length)\n",
    "            stop_frame = int(stop_sample / self.feature_extractor.hop_length)\n",
    "            ph_id = self.phone2id.get(ph, 0)  # 0 может быть индекс padding или unknown\n",
    "            # Присвоим этот id фонемы в маску targets для соответствующих кадров\n",
    "            targets[start_frame:stop_frame] = ph_id\n",
    "        \n",
    "        # Возврат словаря с исходными данными и новыми признаками\n",
    "        \n",
    "        return {\"uri\": orig_item[\"uri\"],\n",
    "                \"feats\": feats,\n",
    "                \"targets\": targets, \n",
    "                \"src_key_padding_mask\": torch.zeros(feats.shape[0], dtype=torch.bool)}\n",
    "    \n",
    "    def collate_pad(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Функция объединения элементов в один батч\"\"\"\n",
    "        # TODO \n",
    "        # Реализуйте функцию, которая объединяет несколько item'ов датасета в один батч\n",
    "        # See collate_fn https://pytorch.org/docs/stable/data.html\n",
    "        # Входные данные и маску надо вернуть таком формате, в каком работает с данными torch.nn.Transformer\n",
    "        # targets надо склеить тензор с одной осью. Длина оси будет равна суммарному количеству кадров в батче\n",
    "        # Максимальная длина последовательности среди всех айтемов\n",
    "        max_len = max(item[\"feats\"].shape[0] for item in batch)\n",
    "        batch_size = len(batch)\n",
    "        feat_dim = batch[0][\"feats\"].shape[1]\n",
    "        \n",
    "        # Инициализируем падденные тензоры для батча и маски паддинга\n",
    "        feats = torch.zeros(max_len, batch_size, feat_dim)\n",
    "        src_key_padding_mask = torch.ones(batch_size, max_len, dtype=torch.bool)  # True - padding\n",
    "        \n",
    "        targets_list = []\n",
    "        \n",
    "        for i, item in enumerate(batch):\n",
    "            length = item[\"feats\"].shape[0]\n",
    "            feats[:length, i, :] = item[\"feats\"]\n",
    "            src_key_padding_mask[i, :length] = False\n",
    "            targets_list.append(item[\"targets\"])\n",
    "        \n",
    "        # Склеиваем targets один длинный тензор по времени без паддинга\n",
    "        targets = torch.cat(targets_list, dim=0)\n",
    "        \n",
    "        return {'feats': feats, # (Time, Batch, feats)\n",
    "               'targets': targets, #(SumTime)\n",
    "               'src_key_padding_mask': src_key_padding_mask, #(Batch, Time)\n",
    "               }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_collate_pad():\n",
    "    fe = FeatureExtractor()\n",
    "    test_ds = FeatsPhoneDataset('timit/data/TEST/', feature_extractor=fe, phone2id=PHONE2ID)\n",
    "\n",
    "    for i in range(20):\n",
    "        targets = test_ds[i]['targets']\n",
    "        orig_ph_ali = test_ds.get_phone_ali(i)\n",
    "        targets_set = set(targets.tolist())\n",
    "        orig_set = set([PHONE2ID[ph] for *_, ph in orig_ph_ali])\n",
    "        assert targets_set == orig_set, f\"{i} \\n{targets_set} \\n {orig_set} \\n {orig_ph_ali}\"\n",
    "\n",
    "    items = [test_ds[i] for i in range(30)]\n",
    "    batch = test_ds.collate_pad(items)\n",
    "    assert len(batch['feats'].shape) == 3, batch['feats'].shape\n",
    "    assert batch['feats'].shape[1] == 30, batch['feats'].shape\n",
    "    \n",
    "    assert len(batch['src_key_padding_mask'].shape) == 2, batch['src_key_padding_mask'].shape\n",
    "    assert batch['src_key_padding_mask'].shape[0] == 30, batch['src_key_padding_mask'].shape\n",
    "    assert batch['src_key_padding_mask'].shape[1] == batch['feats'].shape[0], f\"{batch['feats'].shape} {batch['src_key_padding_mask'].shape}\"\n",
    "    \n",
    "    number_nonmasked_frames = (~batch['src_key_padding_mask']).sum()\n",
    "    assert number_nonmasked_frames == len(batch['targets']), f\"{number_nonmasked_frames} != {len(batch['targets'])}\"\n",
    "\n",
    "    accumulated_len = 0\n",
    "    for i, item in enumerate(items):\n",
    "        feats = batch['feats'][:, i, :]\n",
    "        assert torch.isclose(feats.sum(), item['feats'].sum()) , i\n",
    "        src = batch['src_key_padding_mask'][i, :]\n",
    "        cutted_feats = feats[~src]\n",
    "        assert torch.isclose(item['feats'], cutted_feats).all()\n",
    "        cutted_targets = batch['targets'][accumulated_len: accumulated_len + cutted_feats.shape[0]]\n",
    "        assert torch.isclose(cutted_targets, item['targets']).all()\n",
    "        accumulated_len += cutted_feats.shape[0]\n",
    "    print(\"Test 1.c passed\")\n",
    "    \n",
    "test_collate_pad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59eb65-dbbb-4d11-aa26-b2812b04219e",
   "metadata": {},
   "source": [
    "# 2. Акустическая модель\n",
    "\n",
    "Обучим TransformerEncoder из torch решать задачу пофреймовой классификации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AModel(nn.Module):\n",
    "    def __init__(self, feats_dim, out_dim,  dim=128, num_layers=4, ff_dim=256, dropout=0.1, nhead=4, max_len=780):\n",
    "        super().__init__()\n",
    "        self.feats_dim = feats_dim\n",
    "        self.max_len=max_len\n",
    "        self.input_ff = nn.Linear(feats_dim, dim)\n",
    "        self.positional_encoding = nn.Embedding(max_len, dim)\n",
    "        layer = torch.nn.TransformerEncoderLayer(d_model=dim, \n",
    "                                                 nhead=nhead, \n",
    "                                                 dim_feedforward=ff_dim, \n",
    "                                                 dropout=dropout, \n",
    "                                                 batch_first=False)\n",
    "        self.encoder = torch.nn.TransformerEncoder(encoder_layer=layer, num_layers=num_layers)\n",
    "        \n",
    "        self.head = nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, feats, src_key_padding_mask=None, **kwargs):\n",
    "        #TODO \n",
    "        # реализуйте прямой проход модели.\n",
    "        # Фичи подаются на первый ff слой, \n",
    "        # к результату прибавляются позиционные эмбединги.\n",
    "        # Далее фреймы обрабатываются трансформером \n",
    "        # и финализируются с помощью головы\n",
    "        # feats shape: (Time, Batch, feats_dim)\n",
    "        x = self.input_ff(feats)  # (Time, Batch, dim)\n",
    "        \n",
    "        # создаём позиционные индексы для каждого кадра\n",
    "        timesteps = torch.arange(x.size(0), device=x.device)\n",
    "\n",
    "        # получаем позиционные эмбеддинги и добавляем к признакам\n",
    "        pos_emb = self.positional_encoding(timesteps)  # (Time, dim)\n",
    "        x = x + pos_emb.unsqueeze(1)  # (Time, Batch, dim)\n",
    "\n",
    "        x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # (Time, Batch, dim)\n",
    "\n",
    "        logits = self.head(x)  # (Time, Batch, out_dim)\n",
    "        \n",
    "        return logits # (Time, Batch, Phones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828458c8-e53f-4dcd-b092-00358036cd2a",
   "metadata": {},
   "source": [
    "## 3. Обучение модели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ef5b0-4e9d-4543-9cfa-a449ea5cebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартный пайплайн обучения моделей в pytorch\n",
    "class Trainer(nn.Module):\n",
    "    def __init__(self, model, fe, phone2id, device='cuda', opt_cls=torch.optim.Adam, opt_kwargs={'lr':0.0001}):\n",
    "        super().__init__()\n",
    "        self.device=device\n",
    "        self.fe = fe\n",
    "        self.model = model.to(self.device)\n",
    "        self.phone2id = phone2id\n",
    "        self.id2phone = {i:ph for ph,i in phone2id.items()}\n",
    "        self.optimizer = opt_cls(self.model.parameters(), **opt_kwargs)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        print(f\"{self.model}. {self.device}\")\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        return super().to()\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        batch = self.batch_to_device(batch)\n",
    "        #logits = self.model(**batch)\n",
    "        # TODO\n",
    "        # реализуйте подсчет loss функции  \n",
    "        logits = self.model(batch['feats'], src_key_padding_mask=batch.get('src_key_padding_mask', None))\n",
    "        \n",
    "        # Маска паддинга (Batch, Time)\n",
    "        src_key_padding_mask = batch.get('src_key_padding_mask', None)\n",
    "\n",
    "        # Рассчитаем размеры для корректного паддинга targets \n",
    "        batch_size = src_key_padding_mask.size(0)\n",
    "        max_len = src_key_padding_mask.size(1)\n",
    "        \n",
    "        # Инициализируем targets паддингом -100 (игнорируемый в cross-entropy)\n",
    "        targets_padded = torch.full((batch_size, max_len), fill_value=-100, dtype=torch.long, device=logits.device)\n",
    "        \n",
    "        # Заполняем targets по непаддинговым фреймам из склеенных targets в batch['targets']\n",
    "        offset = 0\n",
    "        for i in range(batch_size):\n",
    "            length = (~src_key_padding_mask[i]).sum().item()  # число непаддинговых фреймов\n",
    "            targets_padded[i, :length] = batch['targets'][offset:offset+length]\n",
    "            offset += length\n",
    "        \n",
    "        # Транспонируем logits к виду (Batch, Time, Classes) и потом делаем (Batch*Time, Classes)\n",
    "        logits = logits.permute(1, 0, 2).contiguous()  # (Batch, Time, Classes)\n",
    "        logits_flat = logits.view(-1, logits.size(-1))\n",
    "        \n",
    "        loss = self.criterion(logits_flat, targets_padded.view(-1))\n",
    "        return loss\n",
    "\n",
    "    def batch_to_device(self, batch):\n",
    "        return {k: v.to(self.device) for k, v in batch.items()}\n",
    "        \n",
    "    def train_one_epoch(self, train_dataloader):\n",
    "        \"\"\" Цикл обучения одной эпохи по всем данным\"\"\"\n",
    "        self.model.train()\n",
    "        pbar = tqdm(train_dataloader)\n",
    "        losses = []\n",
    "        for batch in pbar:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.forward(batch)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            pbar.set_description(f\"training loss {losses[-1]:.5f}\")\n",
    "        return losses\n",
    "\n",
    "    def score(self, valid_dataloader) -> List[float]:\n",
    "        \"\"\"Подсчет лосса на валидационной выборке\"\"\"\n",
    "        pbar = tqdm(valid_dataloader, desc=\"Scoring...\")\n",
    "        losses = []\n",
    "        # TODO \n",
    "        # реализуйте функцию, которая подсчитывает лосс на валидационной выборке \n",
    "        # losses должен хранить значение ошибки на каждом батче \n",
    "        with torch.no_grad():\n",
    "            for batch in pbar:\n",
    "                loss = self.forward(batch)\n",
    "                losses.append(loss.item())\n",
    "        \n",
    "        return losses\n",
    "\n",
    "    def fit(self, train_dataloader, epochs, valid_dataloader=None, plot_losses=True):\n",
    "        \"\"\"Запуск обучения на данном dataloader\"\"\"\n",
    "        pbar = tqdm(range(epochs))\n",
    "        per_epoch_train_losses = []\n",
    "        per_epoch_val_losses = []\n",
    "        for e in pbar:\n",
    "            train_loss = np.mean(self.train_one_epoch(train_dataloader))\n",
    "            per_epoch_train_losses.append(train_loss)\n",
    "            if valid_dataloader is not None:\n",
    "                val_loss = np.mean(self.score(valid_dataloader))\n",
    "                per_epoch_val_losses.append(val_loss)\n",
    "            if plot_losses:\n",
    "                display.clear_output()\n",
    "                self.plot_losses(per_epoch_train_losses, per_epoch_val_losses)\n",
    "            else:\n",
    "                val_loss = val_loss if valid_dataloader is not None else float('Nan')\n",
    "                print(f\"train: {train_loss:.5f} | val: {val_loss:.5f}\")\n",
    "        return per_epoch_train_losses, per_epoch_val_losses\n",
    "    \n",
    "    def plot_losses(self, train_losses, val_losses=[]):\n",
    "        plt.title(f\"Train test losses (epoch {len(train_losses)})\")\n",
    "        plt.plot(range(len(train_losses)), train_losses)\n",
    "        if len(val_losses)>0:\n",
    "            assert len(train_losses) == len(val_losses)\n",
    "            plt.plot(range(len(val_losses)), val_losses)\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend([\"train loss\", \"valid loss\"])\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "                 \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8be5c-c1dd-4587-ac67-b74d27f1e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfit_one_batch_check():\n",
    "    # Для проверки работоспособности кода обучения удоно использовать тест модели на overfit \n",
    "    # Для этого запускается обучение на одном батче данных. \n",
    "    # Если код написан правильно, то модель обязана выучить выучить все примеры из этого батча наизусть. \n",
    "    fe = FeatureExtractor()\n",
    "    train_dataset = FeatsPhoneDataset('timit/data/TEST/DR1/FAKS0', feature_extractor=fe, phone2id=PHONE2ID)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, collate_fn=train_dataset.collate_pad)\n",
    "    test_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, collate_fn=train_dataset.collate_pad)\n",
    "\n",
    "    trainer = Trainer(model=AModel(feats_dim=fe.feats_dim, \n",
    "                                   out_dim=max(PHONE2ID.values()) + 1,  \n",
    "                                   dim=256, \n",
    "                                   num_layers=6, \n",
    "                                   ff_dim=512, \n",
    "                                   dropout=0.0, \n",
    "                                   nhead=8), \n",
    "                      fe=fe, \n",
    "                      phone2id=PHONE2ID, device='cuda')\n",
    "   \n",
    "    # only one batch. The model must learn it by heart\n",
    "    losses, val_losses = trainer.fit(train_dataloader, 160, valid_dataloader=test_dataloader, plot_losses=False)\n",
    "\n",
    "    trainer.plot_losses(losses, val_losses)\n",
    "\n",
    "    val_loss = np.mean(trainer.score(test_dataloader))\n",
    "    \n",
    "    assert val_loss < 0.5, f\"{val_loss}. Model doesn't train well\" \n",
    "    print(f\"Test 3.a passed\")\n",
    "overfit_one_batch_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e661a5c9-f31f-4dba-a6cf-1a8ada61d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def experiment():\n",
    "    # Запуск полноценного обучения модели\n",
    "    # TODO: Тюнинг гиперпараметров\n",
    "    fe = FeatureExtractor()\n",
    "    test_dataset = FeatsPhoneDataset('timit/data/TEST/', feature_extractor=fe, phone2id=PHONE2ID)\n",
    "    train_dataset = FeatsPhoneDataset('timit/data/TRAIN/', feature_extractor=fe, phone2id=PHONE2ID)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=40, \n",
    "                                               num_workers=0, collate_fn=train_dataset.collate_pad, shuffle=True)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, \n",
    "                                               num_workers=0, collate_fn=test_dataset.collate_pad, shuffle=False)\n",
    "\n",
    "\n",
    "    trainer = Trainer(model=AModel(feats_dim=fe.feats_dim, \n",
    "                                 out_dim=max(PHONE2ID.values())+1, \n",
    "                                 dim=128, \n",
    "                                 num_layers=7, \n",
    "                                 ff_dim=256, \n",
    "                                 dropout=0.0, \n",
    "                                 nhead=8),\n",
    "                     fe=fe, \n",
    "                     phone2id=PHONE2ID, device='cuda')\n",
    "\n",
    "    trainer.fit(train_dataloader, epochs=40, valid_dataloader=test_dataloader, plot_losses=True)\n",
    "    return trainer.to('cpu')\n",
    "results = experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82fb7a-38f9-4618-9030-aabdd62a3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(results, 'baseline.trainer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df28a9-24e8-405a-ab07-7acef7036a13",
   "metadata": {},
   "source": [
    "# Основное задание (12 баллов)\n",
    "Надо улучшить бейзлайн так, чтобы значение loss на валидации было менее 1.9 \n",
    "\n",
    "**Дополнительное задание** (4 балла): Улучшите loss до 1.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0aa4fd-fb40-43f3-915b-e8f2a447fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trained_model(trainer):\n",
    "    test_dataset = FeatsPhoneDataset('timit/data/TEST/', feature_extractor=trainer.fe, phone2id=trainer.phone2id)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, \n",
    "                                               num_workers=0, collate_fn=test_dataset.collate_pad, shuffle=False)\n",
    "    loss = np.mean(trainer.score(test_dataloader))\n",
    "    print(f\"Test loss is {loss}\")\n",
    "    assert loss < 1.8, \"Main task failed\"\n",
    "    print(f\"Main task is done! (12 points)\")\n",
    "    if loss <= 1.3:\n",
    "        print(f\"Additional task is done! (+4 points)\")\n",
    "test_trained_model(results.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6377613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import os\n",
    "\n",
    "# PHONE2ID должен быть определен заранее\n",
    "\n",
    "class FeatureExtractor(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_rate=16000,\n",
    "        n_fft=512,\n",
    "        hop_length=160,\n",
    "        n_mels=128,\n",
    "        f_max=7600,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels,\n",
    "            f_max=f_max\n",
    "        )\n",
    "\n",
    "    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        device = next(self.mel_spec.parameters()).device if len(list(self.mel_spec.parameters())) > 0 else torch.device('cpu')\n",
    "        waveform = waveform.to(device)\n",
    "        mel = self.mel_spec(waveform)\n",
    "        mel = torch.log(mel + 1e-8)\n",
    "        return mel.squeeze(0).transpose(0, 1)\n",
    "\n",
    "    @property\n",
    "    def feats_dim(self):\n",
    "        return self.n_mels\n",
    "\n",
    "\n",
    "class FeatsPhoneDataset(TimitDataset):\n",
    "    def __init__(self, data_path, feature_extractor: FeatureExtractor, phone2id):\n",
    "        super().__init__(data_path)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.phone2id = phone2id\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        orig_item = super().__getitem__(index)\n",
    "        wav, sr = orig_item['audio']\n",
    "        assert sr == self.feature_extractor.sample_rate\n",
    "        feats = self.feature_extractor(wav)\n",
    "\n",
    "        targets = torch.zeros(feats.shape[0], dtype=torch.long)\n",
    "        for start_sample, stop_sample, ph in orig_item['phone_ali']:\n",
    "            start_frame = int(start_sample / self.feature_extractor.hop_length)\n",
    "            stop_frame = int(stop_sample / self.feature_extractor.hop_length)\n",
    "            ph_id = self.phone2id.get(ph, 0)\n",
    "            targets[start_frame:stop_frame] = ph_id\n",
    "\n",
    "        return {\n",
    "            'uri': orig_item['uri'],\n",
    "            'feats': feats,\n",
    "            'targets': targets,\n",
    "            'src_key_padding_mask': torch.zeros(feats.shape[0], dtype=torch.bool)\n",
    "        }\n",
    "\n",
    "    def collate_pad(self, batch):\n",
    "        max_len = max(item['feats'].shape[0] for item in batch)\n",
    "        batch_size = len(batch)\n",
    "        feat_dim = batch[0]['feats'].shape[1]\n",
    "\n",
    "        feats = torch.zeros(max_len, batch_size, feat_dim)\n",
    "        src_key_padding_mask = torch.ones(batch_size, max_len, dtype=torch.bool)\n",
    "        targets_padded = torch.full((batch_size, max_len), fill_value=-100, dtype=torch.long)\n",
    "\n",
    "        for i, item in enumerate(batch):\n",
    "            length = item['feats'].shape[0]\n",
    "            feats[:length, i, :] = item['feats']\n",
    "            src_key_padding_mask[i, :length] = False\n",
    "            targets_padded[i, :length] = item['targets']\n",
    "\n",
    "        return {\n",
    "            'feats': feats,\n",
    "            'targets': targets_padded,\n",
    "            'src_key_padding_mask': src_key_padding_mask\n",
    "        }\n",
    "\n",
    "\n",
    "class AModel(nn.Module):\n",
    "    def __init__(self, feats_dim, out_dim, dim=512, num_layers=8, ff_dim=2048, dropout=0.2, nhead=8, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.input_ff = nn.Linear(feats_dim, dim)\n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(max_len, dim))\n",
    "        nn.init.normal_(self.positional_encoding, mean=0.0, std=0.02)\n",
    "        \n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=False,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=num_layers)\n",
    "        \n",
    "        self.final_norm = nn.LayerNorm(dim)\n",
    "        self.head = nn.Linear(dim, out_dim)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.constant_(module.bias, 0.0)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.constant_(module.bias, 0.0)\n",
    "            torch.nn.init.constant_(module.weight, 1.0)\n",
    "\n",
    "    def forward(self, feats, src_key_padding_mask=None):\n",
    "        x = self.input_ff(feats)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        seq_len = x.size(0)\n",
    "        pos_emb = self.positional_encoding[:seq_len]\n",
    "        x = x + pos_emb.unsqueeze(1)\n",
    "        \n",
    "        x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class Trainer(nn.Module):\n",
    "    def __init__(self, model, fe, phone2id, device='cuda', opt_cls=optim.AdamW, opt_kwargs={'lr': 1e-4}):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.fe = fe\n",
    "        self.model = model.to(device)\n",
    "        self.phone2id = phone2id\n",
    "        self.optimizer = opt_cls(self.model.parameters(), **opt_kwargs)\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        self.scheduler = None\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch = self.batch_to_device(batch)\n",
    "        logits = self.model(batch['feats'], src_key_padding_mask=batch.get('src_key_padding_mask'))\n",
    "        logits = logits.permute(1, 0, 2).contiguous()\n",
    "        logits_flat = logits.view(-1, logits.size(-1))\n",
    "        targets_flat = batch['targets'].view(-1)\n",
    "        loss = self.criterion(logits_flat, targets_flat)\n",
    "        return loss\n",
    "\n",
    "    def batch_to_device(self, batch):\n",
    "        return {k: v.to(self.device) for k, v in batch.items()}\n",
    "\n",
    "    def train_one_epoch(self, dataloader):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        pbar = tqdm(dataloader)\n",
    "        for batch in pbar:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.forward(batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "            losses.append(loss.item())\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            pbar.set_description(f\"Train loss: {loss.item():.4f}, LR: {current_lr:.2e}\")\n",
    "        return losses\n",
    "\n",
    "    def score(self, dataloader):\n",
    "        self.model.eval()\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(dataloader, desc=\"Eval\")\n",
    "            for batch in pbar:\n",
    "                loss = self.forward(batch)\n",
    "                losses.append(loss.item())\n",
    "        return losses\n",
    "\n",
    "    def fit(self, train_dataloader, epochs, valid_dataloader=None, plot_losses=True):\n",
    "        # Используем CosineAnnealingWarmRestarts вместо OneCycleLR\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer, \n",
    "            T_0=10,  # Период первой рестарта\n",
    "            T_mult=2,  # Умножение периода после каждой рестарты\n",
    "            eta_min=1e-6  # Минимальный learning rate\n",
    "        )\n",
    "        \n",
    "        per_epoch_train_losses = []\n",
    "        per_epoch_val_losses = []\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            train_loss = np.mean(self.train_one_epoch(train_dataloader))\n",
    "            per_epoch_train_losses.append(train_loss)\n",
    "            \n",
    "            if valid_dataloader:\n",
    "                val_loss = np.mean(self.score(valid_dataloader))\n",
    "                per_epoch_val_losses.append(val_loss)\n",
    "                \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    torch.save(self.model.state_dict(), 'best_model.pth')\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Validation Loss = {val_loss:.4f}, Best Val Loss = {best_val_loss:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n",
    "\n",
    "            if plot_losses and (epoch + 1) % 5 == 0:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.title(f\"Epoch {epoch+1} Train/Val Loss\")\n",
    "                plt.plot(per_epoch_train_losses, label=\"Train Loss\")\n",
    "                if len(per_epoch_val_losses) > 0:\n",
    "                    plt.plot(per_epoch_val_losses, label=\"Validation Loss\")\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.show()\n",
    "                \n",
    "        if valid_dataloader and os.path.exists('best_model.pth'):\n",
    "            self.model.load_state_dict(torch.load('best_model.pth'))\n",
    "            \n",
    "        return per_epoch_train_losses, per_epoch_val_losses\n",
    "\n",
    "\n",
    "def experiment():\n",
    "    fe = FeatureExtractor()\n",
    "    test_dataset = FeatsPhoneDataset('timit/data/TEST/', feature_extractor=fe, phone2id=PHONE2ID)\n",
    "    train_dataset = FeatsPhoneDataset('timit/data/TRAIN/', feature_extractor=fe, phone2id=PHONE2ID)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=32,\n",
    "        num_workers=4, \n",
    "        collate_fn=train_dataset.collate_pad, \n",
    "        shuffle=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=16,\n",
    "        num_workers=4, \n",
    "        collate_fn=test_dataset.collate_pad, \n",
    "        shuffle=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    model = AModel(\n",
    "        feats_dim=fe.feats_dim,\n",
    "        out_dim=max(PHONE2ID.values()) + 1,\n",
    "        dim=512,\n",
    "        num_layers=8,\n",
    "        ff_dim=2048,\n",
    "        dropout=0.2,\n",
    "        nhead=8\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model, \n",
    "        fe, \n",
    "        PHONE2ID, \n",
    "        device='cuda', \n",
    "        opt_kwargs={\n",
    "            'lr': 2e-4,\n",
    "            'weight_decay': 0.01,\n",
    "            'betas': (0.9, 0.98)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    trainer.fit(train_dataloader, epochs=100, valid_dataloader=test_dataloader)\n",
    "    \n",
    "    if os.path.exists('best_model.pth'):\n",
    "        os.remove('best_model.pth')\n",
    "        \n",
    "    return trainer.to('cpu')\n",
    "\n",
    "\n",
    "results = experiment()\n",
    "\n",
    "\n",
    "def test_trained_model(trainer):\n",
    "    test_dataset = FeatsPhoneDataset('timit/data/TEST/', feature_extractor=trainer.fe, phone2id=trainer.phone2id)\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=1,\n",
    "        num_workers=0, \n",
    "        collate_fn=test_dataset.collate_pad, \n",
    "        shuffle=False\n",
    "    )\n",
    "    loss = np.mean(trainer.score(test_dataloader))\n",
    "    print(f\"Test loss is {loss}\")\n",
    "    assert loss < 1.8, \"Main task failed\"\n",
    "    print(f\"Main task is done! (12 points)\")\n",
    "    if loss <= 1.3:\n",
    "        print(f\"Additional task is done! (+4 points)\")\n",
    "    if loss <= 1.2:\n",
    "        print(f\"Target achieved! Loss <= 1.2\")\n",
    "\n",
    "\n",
    "test_trained_model(results.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873732bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itmo_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
